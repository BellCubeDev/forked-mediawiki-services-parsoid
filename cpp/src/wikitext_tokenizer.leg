%{
#include <vector>
#include <iostream>
#include <sstream>
#include "parsoid_internal.hpp"
using namespace parsoid;
using std::stringstream;
using std::string;
using std::vector;
using std::min;
using std::max;

// Wrap the tokenizer in its own sub-namespace
namespace parsoid{
namespace WTTokenizer {

// Actions are supposed to return this type as semantic value
#define YYSTYPE vector<Tk>

// local tokenizer context
#define YY_CTX_LOCAL

#define YY_INPUT(buf, result, max)  \
    { \
        result = 0; \
    }

// Add a reference to the driving WikiTokenizer object as an additional state
// member
#define YY_CTX_MEMBERS WikiTokenizer* tokenizer;

// If you want to see _ALL THE TEXT_, uncomment this.
//#define YY_DEBUG

// Define a few convenience macros to make things more readable
#define emit ctx->tokenizer->emit
#define pushScope ctx->tokenizer->pushScope
#define popScope ctx->tokenizer->popScope
#define getAccum ctx->tokenizer->getAccum

#define incFlag ctx->tokenizer->syntaxFlags.inc
#define decFlag ctx->tokenizer->syntaxFlags.dec
#define pushFlag ctx->tokenizer->syntaxFlags.push
#define popFlag ctx->tokenizer->syntaxFlags.pop
#define getFlag ctx->tokenizer->syntaxFlags.get

%}


#/*********************************************************
# * The top-level production
# *********************************************************/
start = block

#
# A document (start production) is a sequence of toplevelblocks. Tokens are
# emitted in chunks per toplevelblock to avoid buffering the full document.
#
#toplevelblock = block

# TODO: implement!
block = block_lines
#      | & '<' ( pre
#              | comment
#              | nowiki
#              | bt:block_tag { $$ = bt; } )
#      | paragraph
      | inlineline
      | sol
      | eof

## Old version of block rule
#    newline? l:line
#    { cout << "last block token: " << getAccum()->back().toString() << endl; }

block_lines = sol ( optionalSpaceToken sol )? block_line

block_line = h
#           | lists
           | optionalSpaceToken
#             ( & [{}|!] table_lines
#              ( ( block_tag optionalSpaceToken )+ & eolf )

#list = "*" text { 
#        vector<Token>* v = new vector<Token> { new StartTagTk('listitem') }; 
#        v.push_back( 
#    }


### Headings

h = & "=" # guard, to make sure '='+ will match.
    ( # XXX: Also check to end to avoid inline parsing?
        s:equals # moved in here to make s accessible to inner action
        & { incFlag( WikiTokenizer::SyntaxFlags::Flag::Equal ) }
        c:inlineline
        e:equals
        ( space+ | comment )*
        & eolf
        {
            string start = s[0].getText();
            string end = e[0].getText();

            int level = min( start.size(), end.size() );
            level = min( level, 6 );

            stringstream out;
            out << "h" << level;

            if ( start.size() > level ) {
                c.insert( c.begin(), mkText( string( start.size() - level, '=' ) ) );
            }

            if ( end.size() > level ) {
                c.push_back( mkText( string( end.size() - level, '=' ) ) );
            }

            Tk headStart = mkStartTag( out.str() );
            Tk headEnd = mkEndTag( out.str() );

/*            headStart.setSourceRange( yypos, yypos + start.size() );
            headEnd.setSourceRange( yypos - end.size(), yypos );*/

            emit( headStart );
            emit( c );
            emit( headEnd );
            decFlag( WikiTokenizer::SyntaxFlags::Flag::Equal );
        }
    ) | & { decFlag( WikiTokenizer::SyntaxFlags::Flag::Equal ) }

paragraph = sol sol inlineline

equals = < "="+ > { $$ = vector<Tk>( 1, mkText( yytext ) ); }

inlineline = < ( & { !ctx->tokenizer->syntaxBreak() } [^\r\n] )+ > { $$ = vector<Tk>( 1, mkText( yytext ) ); }

# Start of line
sol = innersol comment* newline?

innersol = & { ctx->pos == 0 }
#         | newlineToken

optionalSpaceToken = space+

line = [^\n]

space = [ \t]

eolf = newline | eof

comment = "<!--" comment_chars* ( "-->" | eof )

comment_chars = !"-->"

eof = !. { emit( mkEof() ); }

newline = '\n' | '\r\n'

# TODO: use a stack of accumulators
# Need stack to support nesting (inline link content within link token for
# example), nested_block, nested_inlineline, nested_inline..
#
# * Start a new current accumulator, push old one on stack
# * In main action: pop & concat
StartChunk = &. { pushScope(); }

%%

} // namespace WTTokenizer
} // namespace parsoid
