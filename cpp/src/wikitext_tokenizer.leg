%{
#include <vector>
#include <iostream>
#include <sstream>
#include "wikitext_tokenizer.hpp"

namespace parsoid {
namespace WTTokenizer {
%}


#/*********************************************************
# * The top-level production
# *********************************************************/
start
    = block* newline* {
        emit( mkEof() );
    }

#
# A document (start production) is a sequence of toplevelblocks. Tokens are
# emitted in chunks per toplevelblock to avoid buffering the full document.
#
#toplevelblock = block

# TODO: implement!
block = block_lines
#      | & '<' ( pre
#              | comment
#              | nowiki
#              | bt:block_tag { $$ = bt; } )
#      | paragraph
      | inlineline
      | sol
      | eof

## Old version of block rule
#    newline? l:line
#    { cout << "last block token: " << getAccum()->back().toString() << endl; }

block_lines = sol ( optionalSpaceToken sol )? block_line

block_line = h
#           | lists
           | optionalSpaceToken
#             ( & [{}|!] table_lines
#              ( ( block_tag optionalSpaceToken )+ & eolf )

#list = "*" text { 
#        vector<Token>* v = new vector<Token> { new StartTagTk('listitem') }; 
#        v.push_back( 
#    }


### Headings

h = & "=" # guard, to make sure '='+ will match.
    ( # XXX: Also check to end to avoid inline parsing?
        s:equals # moved in here to make s accessible to inner action
        & { incFlag( WikiTokenizer::SyntaxFlags::Flag::Equal ) }
        c:inlineline
        e:equals
        ( space+ | comment )*
        & eolf
        {
            string start = s[0].getText();
            string end = e[0].getText();

            unsigned int level = std::min( (unsigned int)start.size()
                                , (unsigned int)end.size() );
            level = std::min( level, (unsigned int)6 );

            stringstream out;
            out << "h" << level;

            if ( start.size() > level ) {
                c.insert( c.begin(), mkText( string( start.size() - level, '=' ) ) );
            }

            if ( end.size() > level ) {
                c.push_back( mkText( string( end.size() - level, '=' ) ) );
            }

            Tk headStart = mkStartTag( out.str() );
            Tk headEnd = mkEndTag( out.str() );

/*            headStart.setSourceRange( yypos, yypos + start.size() );
            headEnd.setSourceRange( yypos - end.size(), yypos );*/

            emit( headStart );
            emit( c );
            emit( headEnd );
            decFlag( WikiTokenizer::SyntaxFlags::Flag::Equal );
        }
    ) | & { decFlag( WikiTokenizer::SyntaxFlags::Flag::Equal ) }

paragraph = sol sol inlineline

equals = < "="+ > { $$ = vector<Tk>( 1, mkText( yytext ) ); }

inlineline = < ( & { !ctx->tokenizer->syntaxBreak() } [^\r\n] )+ > { $$ = vector<Tk>( 1, mkText( yytext ) ); }

# Start of line
sol = innersol comment* newline?

innersol = & { ctx->pos == 0 }
#         | newlineToken

optionalSpaceToken = space+

line = [^\n]

space = [ \t]

eolf = newline | eof

comment = "<!--" comment_chars* ( "-->" | eof )

comment_chars = !"-->"

eof = !. { emit( mkEof() ); }

newline = '\n' | '\r\n'

# TODO: use a stack of accumulators
# Need stack to support nesting (inline link content within link token for
# example), nested_block, nested_inlineline, nested_inline..
#
# * Start a new current accumulator, push old one on stack
# * In main action: pop & concat
StartChunk = &. { pushScope(); }

%%

} // namespace WTTokenizer
} // namespace parsoid
